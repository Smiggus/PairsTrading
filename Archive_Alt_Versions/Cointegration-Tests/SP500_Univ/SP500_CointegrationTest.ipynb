{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Get S&P500 Constituents Data'''\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "# Get the list of S&P 500 constituents\n",
    "# You can get this data from a static list or by scraping a website such as Wikipedia\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'\n",
    "sp500_table = pd.read_html(url)\n",
    "sp500_symbols = sp500_table[0]['Symbol'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[********************  42%%                      ]  213 of 503 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$BF.B: possibly delisted; No price data found  (1d 2018-01-01 -> 2023-01-01)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  503 of 503 completed\n",
      "\n",
      "6 Failed downloads:\n",
      "['SOLV', 'KVUE', 'GEV', 'VLTO']: YFChartError(\"%ticker%: Data doesn't exist for startDate = 1514782800, endDate = 1672549200\")\n",
      "['BF.B']: YFPricesMissingError('$%ticker%: possibly delisted; No price data found  (1d 2018-01-01 -> 2023-01-01)')\n",
      "['BRK.B']: YFTzMissingError('$%ticker%: possibly delisted; No timezone found')\n"
     ]
    }
   ],
   "source": [
    "'''Download Historical Data'''\n",
    "import yfinance as yf\n",
    "\n",
    "# Define the date range\n",
    "start_date = '2018-01-01'\n",
    "end_date = '2023-01-01'\n",
    "\n",
    "# Download historical data for all S&P 500 constituents\n",
    "data = yf.download(sp500_symbols, start=start_date, end=end_date)['Adj Close']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MP\\AppData\\Local\\Temp\\ipykernel_10292\\2448819539.py:3: FutureWarning: The default fill_method='pad' in DataFrame.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  returns = data.pct_change().dropna()\n"
     ]
    }
   ],
   "source": [
    "'''Calcualte Returns'''\n",
    "# Calculate daily returns\n",
    "returns = data.pct_change().dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[**********************91%%******************    ]  459 of 503 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$BF.B: possibly delisted; No price data found  (1d 2018-01-01 -> 2023-01-01)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  503 of 503 completed\n",
      "\n",
      "6 Failed downloads:\n",
      "['GEV', 'VLTO', 'KVUE', 'SOLV']: YFChartError(\"%ticker%: Data doesn't exist for startDate = 1514782800, endDate = 1672549200\")\n",
      "['BRK.B']: YFTzMissingError('$%ticker%: possibly delisted; No timezone found')\n",
      "['BF.B']: YFPricesMissingError('$%ticker%: possibly delisted; No price data found  (1d 2018-01-01 -> 2023-01-01)')\n",
      "C:\\Users\\MP\\AppData\\Local\\Temp\\ipykernel_65260\\2070158949.py:24: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  data.fillna(method='ffill', inplace=True)\n",
      "C:\\Users\\MP\\AppData\\Local\\Temp\\ipykernel_65260\\2070158949.py:25: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  data.fillna(method='bfill', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from statsmodels.tsa.stattools import coint\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get S&P 500 constituents\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'\n",
    "sp500_table = pd.read_html(url)\n",
    "sp500_symbols = sp500_table[0]['Symbol'].tolist()\n",
    "\n",
    "# Download historical data\n",
    "start_date = '2018-01-01'\n",
    "end_date = '2023-01-01'\n",
    "data = yf.download(sp500_symbols, start=start_date, end=end_date)['Adj Close']\n",
    "\n",
    "# Drop tickers with too many missing values\n",
    "missing_threshold = 0.1  # Drop tickers with more than 10% missing data\n",
    "data = data.dropna(thresh=int(data.shape[0] * (1 - missing_threshold)), axis=1)\n",
    "\n",
    "# Handle remaining missing data: fill forward, then fill backward\n",
    "data.fillna(method='ffill', inplace=True)\n",
    "data.fillna(method='bfill', inplace=True)\n",
    "#data.fillna().ffill(inplace=True)\n",
    "#data.fillna().bfill(inplace=True)\n",
    "\n",
    "# Ensure no inf or nan values are present\n",
    "data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Check if the data is empty after cleaning\n",
    "if data.empty:\n",
    "    raise ValueError(\"Data is empty after cleaning. Please check the data source and handling steps.\")\n",
    "\n",
    "# Calculate daily returns\n",
    "returns = data.pct_change().dropna()\n",
    "\n",
    "# Check if the returns DataFrame is empty\n",
    "if returns.empty:\n",
    "    raise ValueError(\"Returns data is empty after calculating percentage changes. Please check the data source and handling steps.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m p_value_matrix, pairs\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Find cointegrated pairs\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m p_value_matrix, pairs \u001b[38;5;241m=\u001b[39m \u001b[43mfind_cointegrated_pairs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Perform PCA for dimensionality reduction\u001b[39;00m\n\u001b[0;32m     24\u001b[0m pca \u001b[38;5;241m=\u001b[39m PCA(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "Cell \u001b[1;32mIn[9], line 13\u001b[0m, in \u001b[0;36mfind_cointegrated_pairs\u001b[1;34m(data, significance_level)\u001b[0m\n\u001b[0;32m     11\u001b[0m stock1 \u001b[38;5;241m=\u001b[39m data[keys[i]]\n\u001b[0;32m     12\u001b[0m stock2 \u001b[38;5;241m=\u001b[39m data[keys[j]]\n\u001b[1;32m---> 13\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mcoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstock1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstock2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m p_value \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     15\u001b[0m p_value_matrix[i, j] \u001b[38;5;241m=\u001b[39m p_value\n",
      "File \u001b[1;32mc:\\Users\\MP\\anaconda3\\envs\\algo\\lib\\site-packages\\statsmodels\\tsa\\stattools.py:1806\u001b[0m, in \u001b[0;36mcoint\u001b[1;34m(y0, y1, trend, method, maxlag, autolag, return_results)\u001b[0m\n\u001b[0;32m   1803\u001b[0m res_co \u001b[38;5;241m=\u001b[39m OLS(y0, xx)\u001b[38;5;241m.\u001b[39mfit()\n\u001b[0;32m   1805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res_co\u001b[38;5;241m.\u001b[39mrsquared \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m*\u001b[39m SQRTEPS:\n\u001b[1;32m-> 1806\u001b[0m     res_adf \u001b[38;5;241m=\u001b[39m \u001b[43madfuller\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1807\u001b[0m \u001b[43m        \u001b[49m\u001b[43mres_co\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxlag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxlag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautolag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautolag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m   1808\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1809\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1810\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1811\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my0 and y1 are (almost) perfectly colinear.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1812\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCointegration test is not reliable in this case.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1813\u001b[0m         CollinearityWarning,\n\u001b[0;32m   1814\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m   1815\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\MP\\anaconda3\\envs\\algo\\lib\\site-packages\\statsmodels\\tsa\\stattools.py:326\u001b[0m, in \u001b[0;36madfuller\u001b[1;34m(x, maxlag, regression, autolag, store, regresults)\u001b[0m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;66;03m# 1 for level\u001b[39;00m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;66;03m# search for lag length with smallest information criteria\u001b[39;00m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;66;03m# Note: use the same number of observations to have comparable IC\u001b[39;00m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;66;03m# aic and bic: smaller is better\u001b[39;00m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m regresults:\n\u001b[1;32m--> 326\u001b[0m     icbest, bestlag \u001b[38;5;241m=\u001b[39m \u001b[43m_autolag\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mOLS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxdshort\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfullRHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstartlag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxlag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautolag\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     icbest, bestlag, alres \u001b[38;5;241m=\u001b[39m _autolag(\n\u001b[0;32m    331\u001b[0m         OLS,\n\u001b[0;32m    332\u001b[0m         xdshort,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    337\u001b[0m         regresults\u001b[38;5;241m=\u001b[39mregresults,\n\u001b[0;32m    338\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\MP\\anaconda3\\envs\\algo\\lib\\site-packages\\statsmodels\\tsa\\stattools.py:133\u001b[0m, in \u001b[0;36m_autolag\u001b[1;34m(mod, endog, exog, startlag, maxlag, method, modargs, fitargs, regresults)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lag \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(startlag, startlag \u001b[38;5;241m+\u001b[39m maxlag \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    132\u001b[0m     mod_instance \u001b[38;5;241m=\u001b[39m mod(endog, exog[:, :lag], \u001b[38;5;241m*\u001b[39mmodargs)\n\u001b[1;32m--> 133\u001b[0m     results[lag] \u001b[38;5;241m=\u001b[39m \u001b[43mmod_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maic\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    136\u001b[0m     icbest, bestlag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m((v\u001b[38;5;241m.\u001b[39maic, k) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mitems())\n",
      "File \u001b[1;32mc:\\Users\\MP\\anaconda3\\envs\\algo\\lib\\site-packages\\statsmodels\\regression\\linear_model.py:336\u001b[0m, in \u001b[0;36mRegressionModel.fit\u001b[1;34m(self, method, cov_type, cov_kwds, use_t, **kwargs)\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpinv\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpinv_wexog\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    333\u001b[0m             \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnormalized_cov_params\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    334\u001b[0m             \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[1;32m--> 336\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpinv_wexog, singular_values \u001b[38;5;241m=\u001b[39m \u001b[43mpinv_extended\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwexog\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    337\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalized_cov_params \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(\n\u001b[0;32m    338\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpinv_wexog, np\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpinv_wexog))\n\u001b[0;32m    340\u001b[0m         \u001b[38;5;66;03m# Cache these singular values for use later.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\MP\\anaconda3\\envs\\algo\\lib\\site-packages\\statsmodels\\tools\\tools.py:264\u001b[0m, in \u001b[0;36mpinv_extended\u001b[1;34m(x, rcond)\u001b[0m\n\u001b[0;32m    262\u001b[0m x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(x)\n\u001b[0;32m    263\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mconjugate()\n\u001b[1;32m--> 264\u001b[0m u, s, vt \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msvd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    265\u001b[0m s_orig \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcopy(s)\n\u001b[0;32m    266\u001b[0m m \u001b[38;5;241m=\u001b[39m u\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\MP\\anaconda3\\envs\\algo\\lib\\site-packages\\numpy\\linalg\\linalg.py:1681\u001b[0m, in \u001b[0;36msvd\u001b[1;34m(a, full_matrices, compute_uv, hermitian)\u001b[0m\n\u001b[0;32m   1678\u001b[0m         gufunc \u001b[38;5;241m=\u001b[39m _umath_linalg\u001b[38;5;241m.\u001b[39msvd_n_s\n\u001b[0;32m   1680\u001b[0m signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD->DdD\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isComplexType(t) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124md->ddd\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m-> 1681\u001b[0m u, s, vh \u001b[38;5;241m=\u001b[39m \u001b[43mgufunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1682\u001b[0m u \u001b[38;5;241m=\u001b[39m u\u001b[38;5;241m.\u001b[39mastype(result_t, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1683\u001b[0m s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mastype(_realType(result_t), copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Function to perform the cointegration test\n",
    "def find_cointegrated_pairs(data, significance_level=0.05):\n",
    "    n = data.shape[1]\n",
    "    if n < 2:\n",
    "        raise ValueError(\"Not enough data to perform cointegration test.\")\n",
    "    p_value_matrix = np.zeros((n, n))\n",
    "    keys = data.columns\n",
    "    pairs = []\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            stock1 = data[keys[i]]\n",
    "            stock2 = data[keys[j]]\n",
    "            result = coint(stock1, stock2)\n",
    "            p_value = result[1]\n",
    "            p_value_matrix[i, j] = p_value\n",
    "            if p_value < significance_level:\n",
    "                pairs.append((keys[i], keys[j]))\n",
    "    return p_value_matrix, pairs\n",
    "\n",
    "# Find cointegrated pairs\n",
    "p_value_matrix, pairs = find_cointegrated_pairs(data)\n",
    "\n",
    "# Perform PCA for dimensionality reduction\n",
    "pca = PCA(n_components=2)\n",
    "returns_pca = pca.fit_transform(returns.T)\n",
    "\n",
    "# Perform KMeans clustering\n",
    "kmeans = KMeans(n_clusters=10)\n",
    "kmeans.fit(returns_pca)\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# Visualize the clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(returns_pca[:, 0], returns_pca[:, 1], c=labels, cmap='viridis')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.title('Clustering of S&P 500 Stocks Based on Returns')\n",
    "plt.show()\n",
    "\n",
    "# Function to get pairs from clusters\n",
    "def get_pairs_from_clusters(labels, data, significance_level=0.05):\n",
    "    unique_labels = np.unique(labels)\n",
    "    pairs = []\n",
    "    for label in unique_labels:\n",
    "        cluster_stocks = data.columns[labels == label]\n",
    "        cluster_data = data[cluster_stocks]\n",
    "        if cluster_data.shape[1] < 2:\n",
    "            continue  # Skip clusters with less than 2 stocks\n",
    "        _, cluster_pairs = find_cointegrated_pairs(cluster_data, significance_level)\n",
    "        pairs.extend(cluster_pairs)\n",
    "    return pairs\n",
    "\n",
    "# Get pairs from clusters\n",
    "clustered_pairs = get_pairs_from_clusters(labels, data)\n",
    "\n",
    "print(f\"Identified pairs: {clustered_pairs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
